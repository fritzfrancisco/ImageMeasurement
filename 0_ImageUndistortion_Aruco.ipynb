{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87cca551-c401-4bd3-9555-b2cfed1e80e6",
   "metadata": {},
   "source": [
    "# Image Undistortion using Charuco Board "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b05d67-cfb3-4912-a912-1e4c8c31f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System information:\n",
    "# - Linux Mint 18.1 Cinnamon 64-bit\n",
    "# - Python 2.7 with OpenCV 3.2.0\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cv2 import aruco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c65b70-0c7f-474b-ad63-eb705530ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_glob_pattern = (\n",
    "    \"/home/fritz/Pictures/olympus_undistortion_test/checkerboard_images/31072024/*.JPG\"\n",
    ")\n",
    "\n",
    "# ChAruco board variables\n",
    "CHARUCOBOARD_ROWCOUNT = 8\n",
    "CHARUCOBOARD_COLCOUNT = 11\n",
    "SQUARE_LENGTH = 0.015\n",
    "MARKER_LENGTH = 0.011\n",
    "aruco_dict = cv2.aruco.DICT_4X4_1000\n",
    "ARUCO_DICT = cv2.aruco.getPredefinedDictionary(aruco_dict)\n",
    "\n",
    "parameters = cv2.aruco.DetectorParameters()\n",
    "parameters.cornerRefinementMethod = aruco.CORNER_REFINE_SUBPIX\n",
    "detector = cv2.aruco.ArucoDetector(ARUCO_DICT, parameters)\n",
    "\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9912399-668c-4d20-b428-7e832b171fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create constants to be passed into OpenCV and Aruco methods\n",
    "CHARUCO_BOARD = cv2.aruco.CharucoBoard(\n",
    "    (CHARUCOBOARD_COLCOUNT, CHARUCOBOARD_ROWCOUNT),\n",
    "    SQUARE_LENGTH,\n",
    "    MARKER_LENGTH,\n",
    "    ARUCO_DICT,\n",
    ")\n",
    "\n",
    "# Create the arrays and variables we'll use to store info like corners and IDs from images processed\n",
    "corners_all = []  # Corners discovered in all images processed\n",
    "ids_all = []  # Aruco ids corresponding to corners discovered\n",
    "image_size = None  # Determined at runtime\n",
    "\n",
    "# This requires a set of images or a video taken with the camera you want to calibrate\n",
    "# I'm using a set of images taken with the camera with the naming convention:\n",
    "# 'camera-pic-of-charucoboard-<NUMBER>.jpg'\n",
    "# All images used should be the same size, which if taken with the same camera shouldn't be a problem\n",
    "images = glob.glob(filename_glob_pattern)\n",
    "\n",
    "# Loop through images glob'ed\n",
    "for iname in images:\n",
    "    # Open the image\n",
    "    img = cv2.imread(iname)\n",
    "    height, width, _ = img.shape\n",
    "    # Grayscale the image\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find aruco markers in the query image\n",
    "    # corners, ids, _ = aruco.detectMarkers(image=gray, dictionary=ARUCO_DICT)\n",
    "    corners, ids, _ = detector.detectMarkers(image=gray)\n",
    "\n",
    "    # Outline the aruco markers found in our query image\n",
    "    img = aruco.drawDetectedMarkers(image=img, corners=corners)\n",
    "\n",
    "    # Get charuco corners and ids from detected aruco markers\n",
    "    response, charuco_corners, charuco_ids = aruco.interpolateCornersCharuco(\n",
    "        markerCorners=corners, markerIds=ids, image=gray, board=CHARUCO_BOARD\n",
    "    )\n",
    "\n",
    "    if charuco_corners is None:\n",
    "        continue\n",
    "\n",
    "    # Set the needed parameters to find the refined corners\n",
    "    winSize = (5, 5)\n",
    "    zeroZone = (-1, -1)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "    # Calculate the refined corner locations\n",
    "    charuco_corners = cv2.cornerSubPix(\n",
    "        gray, charuco_corners, winSize, zeroZone, criteria\n",
    "    )\n",
    "\n",
    "    # If a Charuco board was found, let's collect image/corner points\n",
    "    # Requiring at least 20 squares\n",
    "    if response > 20:\n",
    "        # Add these corners and ids to our calibration arrays\n",
    "        corners_all.append(charuco_corners)\n",
    "        ids_all.append(charuco_ids)\n",
    "\n",
    "        # Draw the Charuco board we've detected to show our calibrator the board was properly detected\n",
    "        img = aruco.drawDetectedCornersCharuco(\n",
    "            image=img, charucoCorners=charuco_corners, charucoIds=charuco_ids\n",
    "        )\n",
    "\n",
    "        # If our image size is unknown, set it now\n",
    "        if not image_size:\n",
    "            image_size = gray.shape[::-1]\n",
    "\n",
    "        # Reproportion the image, maxing width or height at 1000\n",
    "        proportion = max(img.shape) / 1000.0\n",
    "        img = cv2.resize(\n",
    "            img, (int(img.shape[1] / proportion), int(img.shape[0] / proportion))\n",
    "        )\n",
    "        # Pause to display each image, waiting for key press\n",
    "        cv2.imshow(\"Charuco board\", img)\n",
    "        cv2.waitKey(1)\n",
    "    else:\n",
    "        print(\"Not able to detect a charuco board in image: {}\".format(iname))\n",
    "\n",
    "# Destroy any open CV windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Make sure at least one image was found\n",
    "if len(images) < 1:\n",
    "    # Calibration failed because there were no images, warn the user\n",
    "    print(\n",
    "        \"Calibration was unsuccessful. No images of charucoboards were found. Add images of charucoboards and use or alter the naming conventions used in this file.\"\n",
    "    )\n",
    "    # Exit for failure\n",
    "    exit()\n",
    "\n",
    "# Make sure we were able to calibrate on at least one charucoboard by checking\n",
    "# if we ever determined the image size\n",
    "if not image_size:\n",
    "    # Calibration failed because we didn't see any charucoboards of the PatternSize used\n",
    "    print(\n",
    "        \"Calibration was unsuccessful. We couldn't detect charucoboards in any of the images supplied. Try changing the patternSize passed into Charucoboard_create(), or try different pictures of charucoboards.\"\n",
    "    )\n",
    "    # Exit for failure\n",
    "    exit()\n",
    "\n",
    "# Now that we've seen all of our images, perform the camera calibration\n",
    "# based on the set of points we've discovered\n",
    "calibration, cameraMatrix, distCoeffs, rvecs, tvecs = aruco.calibrateCameraCharuco(\n",
    "    charucoCorners=corners_all,\n",
    "    charucoIds=ids_all,\n",
    "    board=CHARUCO_BOARD,\n",
    "    imageSize=image_size,\n",
    "    cameraMatrix=None,\n",
    "    distCoeffs=None,\n",
    ")\n",
    "\n",
    "newcameraMatrix, _ = cv2.getOptimalNewCameraMatrix(\n",
    "    cameraMatrix, distCoeffs, (width, height), 1, (width, height)\n",
    ")\n",
    "\n",
    "# Print matrix and distortion coefficient to the console\n",
    "print(newcameraMatrix)\n",
    "print(distCoeffs)\n",
    "\n",
    "# Save values to be used where matrix+dist is required, for instance for posture estimation\n",
    "# I save files in a pickle file, but you can use yaml or whatever works for you\n",
    "f = open(\"calibration.pckl\", \"wb\")\n",
    "pickle.dump((newcameraMatrix, cameraMatrix, distCoeffs, rvecs, tvecs), f)\n",
    "f.close()\n",
    "\n",
    "# Print to console our success\n",
    "print(\"Calibration successful. Calibration file used: {}\".format(\"calibration.pckl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65410c70-fd9e-4650-bbf1-72d1fa095fd7",
   "metadata": {},
   "source": [
    "## Read out saved values\n",
    "\n",
    "newcameraMatrix, distCoeffs, rvecs, tvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb00a8a-cf16-4d27-b38f-726ad4d7f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration data\n",
    "newcameraMatrix, cameraMatrix, distCoeffs, rvecs, tvecs = pd.read_pickle(\n",
    "    \"calibration.pckl\"\n",
    ")\n",
    "\n",
    "# Images\n",
    "filename_glob_pattern = \"/home/fritz/Pictures/olympus_undistortion_test/aruco_measurement_images/Tag_0/*.JPG\"\n",
    "\n",
    "# Output path\n",
    "output_dir = \"/home/fritz/Pictures/olympus_undistortion_test/fish_images/undistorted/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ace70e-e6dc-42ed-ba24-81df74b29f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multical\n",
    "f = open(\n",
    "    \"/home/fritz/Pictures/olympus_undistortion_test/checkerboard_images/31072024/calibration.json\"\n",
    ")\n",
    "data = json.load(f)\n",
    "cameraMatrix = np.array(data[\"cameras\"][\"cam1\"][\"K\"])\n",
    "distCoeffs = np.array(data[\"cameras\"][\"cam1\"][\"dist\"]).reshape(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d750e8-6678-43f3-acc6-77893e1ab4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob.glob(filename_glob_pattern)\n",
    "\n",
    "cv2.namedWindow(\"Undistorted Image\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "if os.path.isdir(output_dir) == False:\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for img in images:\n",
    "    filename = output_dir + Path(img).stem + \"_undistorted\" + Path(img).suffix\n",
    "    print(filename)\n",
    "    img = cv2.imread(img)\n",
    "\n",
    "    # Undistort the image\n",
    "    h, w = img.shape[:2]\n",
    "    new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(\n",
    "        cameraMatrix, distCoeffs, (w, h), 1, (w, h)\n",
    "    )\n",
    "    undistorted_image = cv2.undistort(\n",
    "        img, cameraMatrix, distCoeffs, None, new_camera_matrix\n",
    "    )\n",
    "\n",
    "    # Crop the image (if desired, based on ROI)\n",
    "    x, y, w, h = roi\n",
    "    undistorted_image = undistorted_image[y : y + h, x : x + w]\n",
    "\n",
    "    cv2.imwrite(filename, undistorted_image)\n",
    "    cv2.imshow(\"Undistorted Image\", undistorted_image)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23cad1b-f142-4935-a147-b1166e3ec9c6",
   "metadata": {},
   "source": [
    "## Warp Transform using Aruco Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23509b3a-38b7-4d07-bbee-031d718e5d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def rectify_image_using_aruco(\n",
    "    image_path, cameraMatrix, distCoeffs, aruco_dict_type=cv2.aruco.DICT_4X4_100\n",
    "):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Undistort the image\n",
    "    h, w = img.shape[:2]\n",
    "    new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(\n",
    "        cameraMatrix, distCoeffs, (w, h), 1, (w, h)\n",
    "    )\n",
    "    undistorted_image = cv2.undistort(\n",
    "        img, cameraMatrix, distCoeffs, None, new_camera_matrix\n",
    "    )\n",
    "\n",
    "    # Crop the image (if desired, based on ROI)\n",
    "    x, y, w, h = roi\n",
    "    img = undistorted_image[y : y + h, x : x + w]\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Load the dictionary that was used to generate the markers\n",
    "    aruco_dict = cv2.aruco.getPredefinedDictionary(aruco_dict_type)\n",
    "    # Initialize the detector parameters using default values\n",
    "    parameters = cv2.aruco.DetectorParameters()\n",
    "    detector = cv2.aruco.ArucoDetector(aruco_dict, parameters)\n",
    "\n",
    "    # Detect the markers in the image\n",
    "    corners, ids, rejected_img_points = cv2.aruco.detectMarkers(\n",
    "        gray, aruco_dict, parameters=parameters\n",
    "    )\n",
    "\n",
    "    if ids is not None:\n",
    "        # Assuming we have at least one marker detected, we will use the first detected marker\n",
    "        corners = corners[0].reshape((4, 2))\n",
    "\n",
    "        # Define the destination points for the perspective transform\n",
    "        (top_left, top_right, bottom_right, bottom_left) = corners\n",
    "\n",
    "        width = max(\n",
    "            int(np.linalg.norm(bottom_right - bottom_left)),\n",
    "            int(np.linalg.norm(top_right - top_left)),\n",
    "        )\n",
    "        height = max(\n",
    "            int(np.linalg.norm(top_right - bottom_right)),\n",
    "            int(np.linalg.norm(top_left - bottom_left)),\n",
    "        )\n",
    "\n",
    "        dst_pts = np.array(\n",
    "            [\n",
    "                [img.shape[1] - (width - 1), img.shape[0] / 4],\n",
    "                [img.shape[1], img.shape[0] / 4],\n",
    "                [img.shape[1], (img.shape[0] / 4) + (height - 1)],\n",
    "                [img.shape[1] - (width - 1), (img.shape[0] / 4) + (height - 1)],\n",
    "            ],\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "        # Get the perspective transform matrix\n",
    "        M = cv2.getPerspectiveTransform(corners, dst_pts, cv2.WARP_INVERSE_MAP)\n",
    "\n",
    "        # Apply the perspective transformation to get the rectified image\n",
    "        rectified_img = cv2.warpPerspective(\n",
    "            img, M, (img.shape[1], img.shape[0]), flags=cv2.INTER_LINEAR\n",
    "        )\n",
    "\n",
    "        return rectified_img\n",
    "    else:\n",
    "        print(\"No ArUco markers detected.\")\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf7658-7d24-4c44-b5ec-87552cd4f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predefined dictionary\n",
    "aruco_dict = cv2.aruco.DICT_4X4_1000\n",
    "dictionary = cv2.aruco.getPredefinedDictionary(aruco_dict)\n",
    "parameters = cv2.aruco.DetectorParameters()\n",
    "detector = cv2.aruco.ArucoDetector(dictionary, parameters)\n",
    "\n",
    "# Measurement images containing charuco tag\n",
    "charuco_measurement_images = sorted(\n",
    "    glob.glob(\n",
    "        \"/home/fritz/Pictures/olympus_undistortion_test/aruco_measurement_images/Tag_0/labeled/*.JPG\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Output dir\n",
    "output_dir = \"/home/fritz/Pictures/olympus_undistortion_test/aruco_measurement_images/Tag_0/undistored_rectified/\"\n",
    "if os.path.isdir(output_dir) == False:\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Multical Calibration Parameters\n",
    "f = open(\n",
    "    \"/home/fritz/Pictures/olympus_undistortion_test/checkerboard_images/31072024/calibration.json\"\n",
    ")\n",
    "data = json.load(f)\n",
    "cameraMatrix = np.array(data[\"cameras\"][\"cam1\"][\"K\"])\n",
    "distCoeffs = np.array(data[\"cameras\"][\"cam1\"][\"dist\"]).reshape(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177581d7-cd62-4f1c-927f-40efac10f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in charuco_measurement_images:\n",
    "    frame = cv2.imread(img)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect markers in the image\n",
    "    # corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(gray, aruco_dict, parameters=parameters)\n",
    "    corners, ids, rejectedImgPoints = detector.detectMarkers(gray)\n",
    "\n",
    "    # If markers are detected\n",
    "    if ids is not None:\n",
    "        # Draw the markers on the frame\n",
    "        cv2.aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "        rectified_image = rectify_image_using_aruco(img, cameraMatrix, distCoeffs)\n",
    "\n",
    "        ## Save to file\n",
    "        filename = output_dir + Path(img).stem + \"_rectified\" + Path(img).suffix\n",
    "        cv2.imwrite(filename, rectified_image)\n",
    "\n",
    "        cv2.namedWindow(\"Frame\", cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(\"Frame\", rectified_image)\n",
    "        cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7482fa7-f64e-48cb-9d72-6a206c818136",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a0d42c-bdc2-47ff-9c3d-aa8b6f62f180",
   "metadata": {},
   "source": [
    "# Segment Anything 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49622467-b4bb-4099-be69-074aa1e7b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "AMD Radeon Graphics\n"
     ]
    }
   ],
   "source": [
    "import torch, grp, pwd, os, subprocess, gc\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b7dba3-7ed8-489e-956a-16065c111f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary: 422 layers, 38,945,986 parameters, 38,945,986 gradients\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: HIPBLAS_STATUS_ALLOC_FAILED when calling `hipblasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39minfo()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# model(\"path/to/video.mp4\")\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mASSETS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/fritz/Pictures/olympus_undistortion_test/aruco_measurement_images/Tag_0/undistored_rectified/RB251_Rank1_00009_P7290010_rectified.JPG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# model(ASSETS / , device=\"cpu)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Clean up\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/ultralytics/models/sam/model.py:98\u001b[0m, in \u001b[0;36mSAM.__call__\u001b[0;34m(self, source, stream, bboxes, points, labels, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, bboxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    Alias for the 'predict' method.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m        (list): The model predictions.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/ultralytics/models/sam/model.py:82\u001b[0m, in \u001b[0;36mSAM.predict\u001b[0;34m(self, source, stream, bboxes, points, labels, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(overrides)\n\u001b[1;32m     81\u001b[0m prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(bboxes\u001b[38;5;241m=\u001b[39mbboxes, points\u001b[38;5;241m=\u001b[39mpoints, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/ultralytics/engine/model.py:563\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/ultralytics/engine/predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/ultralytics/engine/predictor.py:254\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 254\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/ultralytics/models/sam/predict.py:148\u001b[0m, in \u001b[0;36mPredictor.inference\u001b[0;34m(self, im, bboxes, points, labels, masks, multimask_output, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m, masks)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(i \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [bboxes, points, masks]):\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_inference(im, bboxes, points, labels, masks, multimask_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/ultralytics/models/sam/predict.py:268\u001b[0m, in \u001b[0;36mPredictor.generate\u001b[0;34m(self, im, crop_n_layers, crop_overlap_ratio, crop_downscale_factor, point_grids, points_stride, points_batch_size, conf_thres, stability_score_thresh, stability_score_offset, crop_nms_thresh)\u001b[0m\n\u001b[1;32m    266\u001b[0m crop_masks, crop_scores, crop_bboxes \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (points,) \u001b[38;5;129;01min\u001b[39;00m batch_iterator(points_batch_size, points_for_image):\n\u001b[0;32m--> 268\u001b[0m     pred_mask, pred_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrop_im\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;66;03m# Interpolate predicted masks to input size\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     pred_mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(pred_mask[\u001b[38;5;28;01mNone\u001b[39;00m], (h, w), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/ultralytics/models/sam2/predict.py:87\u001b[0m, in \u001b[0;36mSAM2Predictor.prompt_inference\u001b[0;34m(self, im, bboxes, points, labels, masks, multimask_output, img_idx)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprompt_inference\u001b[39m(\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     55\u001b[0m     im,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     img_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     62\u001b[0m ):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    Performs image segmentation inference based on various prompts using SAM2 architecture.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m        >>> masks, scores, logits = predictor.prompt_inference(image, bboxes=bboxes)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_im_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[1;32m     89\u001b[0m     src_shape, dst_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m], im\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     90\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegment_all \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(dst_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m src_shape[\u001b[38;5;241m0\u001b[39m], dst_shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m src_shape[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/ultralytics/models/sam2/predict.py:174\u001b[0m, in \u001b[0;36mSAM2Predictor.get_im_features\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_im_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, im):\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extracts and processes image features using SAM2's image encoder for subsequent segmentation tasks.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m     backbone_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     _, vision_feats, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_prepare_backbone_features(backbone_out)\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdirectly_add_no_mem_embed:\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/ultralytics/models/sam2/modules/sam2.py:454\u001b[0m, in \u001b[0;36mSAM2Model.forward_image\u001b[0;34m(self, img_batch)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, img_batch: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Process image batch through encoder to extract multi-level features for SAM model.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m     backbone_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_high_res_features_in_sam:\n\u001b[1;32m    456\u001b[0m         \u001b[38;5;66;03m# precompute projected level 0 and level 1 features in SAM decoder\u001b[39;00m\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;66;03m# to avoid running it again on every SAM click\u001b[39;00m\n\u001b[1;32m    458\u001b[0m         backbone_out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone_fpn\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msam_mask_decoder\u001b[38;5;241m.\u001b[39mconv_s0(backbone_out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone_fpn\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/ultralytics/models/sam2/modules/encoders.py:78\u001b[0m, in \u001b[0;36mImageEncoder.forward\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Processes image input through trunk and neck, returning features, positional encodings, and FPN outputs.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     features, pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;66;03m# Discard the lowest resolution features\u001b[39;00m\n\u001b[1;32m     81\u001b[0m         features, pos \u001b[38;5;241m=\u001b[39m features[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalp], pos[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalp]\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/ultralytics/models/sam2/modules/encoders.py:327\u001b[0m, in \u001b[0;36mHiera.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    325\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m--> 327\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_ends[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;129;01mor\u001b[39;00m (i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_ends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_interm_layers):\n\u001b[1;32m    329\u001b[0m         feats \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/ultralytics/models/sam2/modules/sam2_blocks.py:606\u001b[0m, in \u001b[0;36mMultiScaleBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    603\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, window_size)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# Window Attention + Q Pooling (if stage change)\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_stride:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;66;03m# Shapes have changed due to Q pooling\u001b[39;00m\n\u001b[1;32m    609\u001b[0m     window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_stride[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/ultralytics/models/sam2/modules/sam2_blocks.py:514\u001b[0m, in \u001b[0;36mMultiScaleAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    512\u001b[0m B, H, W, _ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m# qkv with shape (B, H * W, 3, nHead, C)\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(B, H \u001b[38;5;241m*\u001b[39m W, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# q, k, v with shape (B, H * W, nheads, C)\u001b[39;00m\n\u001b[1;32m    516\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munbind(qkv, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sam/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: HIPBLAS_STATUS_ALLOC_FAILED when calling `hipblasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "from os import putenv\n",
    "from ultralytics import ASSETS, SAM\n",
    "\n",
    "putenv(\"HSA_OVERRIDE_GFX_VERSION\", \"11.0.0\") # override and trick to seem like using 10.3.0\n",
    "putenv(\"PYTORCH_ROCM_ARCH\", \"gfx1100\") # override and trick to seem like using 10.3.0\n",
    "\n",
    "# putenv(\"HSA_OVERRIDE_GFX_VERSION\", \"10.3.0\") # override and trick to seem like using 10.3.0\n",
    "# putenv(\"PYTORCH_ROCM_ARCH\", \"gfx1030\") # override and trick to seem like using 10.3.0\n",
    "# putenv(\"HIP_VISIBLE_DEVICES\",\"0\")\n",
    "putenv(\"PYTORCH_HIP_ALLOC_CONF\",\"garbage_collection_threshold:0.6,max_split_size_mb:128\")\n",
    "# putenv(\"COMMANDLINE_ARGS\", \"--opt-split-attention --no-half-vae --disable-nan-check --autolaunch --listen\")\n",
    "putenv(\"COMMANDLINE_ARGS\", \"--upcast-sampling --opt-sub-quad-attention\")\n",
    "\n",
    "# Load a model\n",
    "model = SAM(\"/home/fritz/Downloads/sam2_t.pt\")\n",
    "\n",
    "# Display model information (optional)\n",
    "model.info()\n",
    "\n",
    "# Run inference\n",
    "# model(\"path/to/video.mp4\")\n",
    "results = model(ASSETS / \"/home/fritz/Pictures/olympus_undistortion_test/aruco_measurement_images/Tag_0/undistored_rectified/RB251_Rank1_00009_P7290010_rectified.JPG\", device=\"cuda\")\n",
    "# model(ASSETS / , device=\"cpu)\n",
    "\n",
    "# Clean up\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e766933-f9d5-4d83-9080-c3a585288c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Visualize the detection\n",
    "img = results[0].plot()  # This plots the detections on the image\n",
    "\n",
    "# Convert BGR to RGB (OpenCV uses BGR by default)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "cv2.namedWindow('Detection Results', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('Detection Results', img_rgb)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e1d83-1860-42f9-b394-eff1a3526837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, grp, pwd, os, subprocess\n",
    "devices = []\n",
    "try:\n",
    "\tprint(\"\\n\\nChecking ROCM support...\")\n",
    "\tresult = subprocess.run(['rocminfo'], stdout=subprocess.PIPE)\n",
    "\tcmd_str = result.stdout.decode('utf-8')\n",
    "\tcmd_split = cmd_str.split('Agent ')\n",
    "\tfor part in cmd_split:\n",
    "\t\titem_single = part[0:1]\n",
    "\t\titem_double = part[0:2]\n",
    "\t\tif item_single.isnumeric() or item_double.isnumeric():\n",
    "\t\t\tnew_split = cmd_str.split('Agent '+item_double)\n",
    "\t\t\tdevice = new_split[1].split('Marketing Name:')[0].replace('  Name:                    ', '').replace('\\n','').replace('                  ','').split('Uuid:')[0].split('*******')[1]\n",
    "\t\t\tdevices.append(device)\n",
    "\tif len(devices) > 0:\n",
    "\t\tprint('GOOD: ROCM devices found: ', len(devices))\n",
    "\telse:\n",
    "\t\tprint('BAD: No ROCM devices found.')\n",
    "\n",
    "\tprint(\"Checking PyTorch...\")\n",
    "\tx = torch.rand(5, 3)\n",
    "\thas_torch = False\n",
    "\tlen_x = len(x)\n",
    "\tif len_x == 5:\n",
    "\t\thas_torch = True\n",
    "\t\tfor i in x:\n",
    "\t\t\tif len(i) == 3:\n",
    "\t\t\t\thas_torch = True\n",
    "\t\t\telse:\n",
    "\t\t\t\thas_torch = False\n",
    "\tif has_torch:\n",
    "\t\tprint('GOOD: PyTorch is working fine.')\n",
    "\telse:\n",
    "\t\tprint('BAD: PyTorch is NOT working.')\n",
    "\n",
    "\n",
    "\tprint(\"Checking user groups...\")\n",
    "\tuser = os.getlogin()\n",
    "\tgroups = [g.gr_name for g in grp.getgrall() if user in g.gr_mem]\n",
    "\tgid = pwd.getpwnam(user).pw_gid\n",
    "\tgroups.append(grp.getgrgid(gid).gr_name)\n",
    "\tif 'render' in groups and 'video' in groups:\n",
    "\t\tprint('GOOD: The user', user, 'is in RENDER and VIDEO groups.')\n",
    "\telse:\n",
    "\t\tprint('BAD: The user', user, 'is NOT in RENDER and VIDEO groups. This is necessary in order to PyTorch use HIP resources')\n",
    "\n",
    "\tif torch.cuda.is_available():\n",
    "\t\tprint(\"GOOD: PyTorch ROCM support found.\")\n",
    "\t\tt = torch.tensor([5, 5, 5], dtype=torch.int64, device='cuda')\n",
    "\t\tprint('Testing PyTorch ROCM support...')\n",
    "\t\tif str(t) == \"tensor([5, 5, 5], device='cuda:0')\":\n",
    "\t\t\tprint('Everything fine! You can run PyTorch code inside of: ')\n",
    "\t\t\tfor device in devices:\n",
    "\t\t\t\tprint('---> ', device)\n",
    "\telse:\n",
    "\t\tprint(\"BAD: PyTorch ROCM support NOT found.\")\n",
    "except:\n",
    "\tprint('Cannot find rocminfo command information. Unable to determine if AMDGPU drivers with ROCM support were installed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b843893e-7e81-4690-94ab-6852c34dc9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
