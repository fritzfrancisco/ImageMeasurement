{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87cca551-c401-4bd3-9555-b2cfed1e80e6",
   "metadata": {},
   "source": [
    "# Image Undistortion using Charuco Board \n",
    "Ideally camera calibration is done using [multical](https://github.com/oliver-batchelor/multical) in order to ensure highest compatibility. \n",
    "\n",
    "## Multical\n",
    "\n",
    "We will used [multical](https://github.com/oliver-batchelor/multical) to retrieve the calibration values for the camera. It is recommended to create a separate ```conda``` environment which we will call ```mcal```. This way the calibration software is contained and isn't affected by other packages.   \n",
    "\n",
    "To do so we first need to follow a few steps:\n",
    "\n",
    "1. **Create Calibration Pattern:** <br> To do so you can use the accompanied ```checkerboard_charuco_8x11.yaml``` file and running the following command in a terminal: <br> ```multical boards --boards checkerboard_charuco_8x11.yaml --write ~/Documents/ --paper_size_mm 216x279```  <br> This will create a calibration pattern which is recognized by [multical](https://github.com/oliver-batchelor/multical). It is advised to keep the ```checkerboard_charuco_8x11.yaml``` in a known location as it is used for the calibration process later on. It is also a good idea to double check and measure the aruco tag and square width on the print, to match the parameters in the ```checkerboard_charuco_8x11.yaml``` file. If they don't match you can create a copy of the file and change the values to match those of the print. \n",
    "2. **Take Pictures of Calibration Pattern:**<br> Now we need to take pictures with the camera that will be used for measuring. Ideally this is done under the same conditions as when measuring. Therefore, if measurement images are taken underwater, with the camera in a casing, the calibration images should be taken under the same circumstances and with ideantical setup. This ensures that the calibration is optimal!\n",
    "3. **Calibrate Camera:** <br> Once we have the calibration images, these should be stored in a folder with standardized name. Here we will use a folder structure of ```DDMMYYYY/cam1``` (for example: ```31072024/cam1```). Then we can run the calibration for a single camera using [multical](https://github.com/oliver-batchelor/multical) and the following command: <br>```multical calibrate --image_path /home/fritz/Pictures/olympus_undistortion_test/data/checkerboard_images/31072024/ --boards /home/fritz/Downloads/charuco_png_test_8x11.yaml --camera_pattern cam1``` <br> This command will create a few output files in the image directory, of which we will use ```calibration.json``` moving forward.\n",
    "\n",
    "## Warp Transform using Aruco Tag\n",
    "This process is also known as rectification and uses a reference shape to transform the image. It is important that this step is done on undistorted images, to reduce error and improve the process. The general process goes as follows:\n",
    "\n",
    "1. An Aruco Tag is attached to a flat surface on which the objects (i.e. Fish) are photographed and later measured. This tag should always be in full view, as well as the objects in the images. It is important to note the tag number and ideally the dictionary with which it was generated with. \n",
    "2. We can then tranform the image to a be as if it was viewed directly from above. This is done by automatically finding the tag in the image and warp transforming it to make the tag square again.\n",
    "3. Once transformed, the images can be saved for use in measuring the objects of interest!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd00cc8-18b7-48ad-bc9f-28dc708137d0",
   "metadata": {},
   "source": [
    "## Create Aruco Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65410c70-fd9e-4650-bbf1-72d1fa095fd7",
   "metadata": {},
   "source": [
    "## Read out saved values\n",
    "\n",
    "newcameraMatrix, distCoeffs, rvecs, tvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34afed-042c-41bf-a54c-03b66c76bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cv2 import aruco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb00a8a-cf16-4d27-b38f-726ad4d7f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images\n",
    "filename_glob_pattern = \"/media/fritz/01_PNG_DATA/MeasurementImages/26042025/labeled/*.JPG\"\n",
    "\n",
    "# Output path\n",
    "output_dir = \"/media/fritz/01_PNG_DATA/MeasurementImages/26042025/undistorted/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ace70e-e6dc-42ed-ba24-81df74b29f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multical\n",
    "f = open(\n",
    "    \"/media/fritz/01_PNG_DATA/MeasurementImages/calibration_images/tg-6/calibration.json\"\n",
    ")\n",
    "data = json.load(f)\n",
    "cameraMatrix = np.array(data[\"cameras\"][\"cam0\"][\"K\"])\n",
    "distCoeffs = np.array(data[\"cameras\"][\"cam0\"][\"dist\"]).reshape(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d750e8-6678-43f3-acc6-77893e1ab4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob.glob(filename_glob_pattern)\n",
    "\n",
    "cv2.namedWindow(\"Undistorted Image\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "if os.path.isdir(output_dir) == False:\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for img in images:\n",
    "    filename = output_dir + Path(img).stem + \"_undistorted\" + Path(img).suffix\n",
    "    print(filename)\n",
    "    img = cv2.imread(img)\n",
    "\n",
    "    # Undistort the image\n",
    "    h, w = img.shape[:2]\n",
    "    new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(\n",
    "        cameraMatrix, distCoeffs, (w, h), 1, (w, h)\n",
    "    )\n",
    "    undistorted_image = cv2.undistort(\n",
    "        img, cameraMatrix, distCoeffs, None, new_camera_matrix\n",
    "    )\n",
    "\n",
    "    # Crop the image (if desired, based on ROI)\n",
    "    x, y, w, h = roi\n",
    "    undistorted_image = undistorted_image[y : y + h, x : x + w]\n",
    "\n",
    "    cv2.imwrite(filename, undistorted_image)\n",
    "    cv2.imshow(\"Undistorted Image\", undistorted_image)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23cad1b-f142-4935-a147-b1166e3ec9c6",
   "metadata": {},
   "source": [
    "## Warp Transform using Aruco Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23509b3a-38b7-4d07-bbee-031d718e5d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def rectify_image_using_aruco(\n",
    "    image_path, cameraMatrix, distCoeffs, aruco_dict_type=cv2.aruco.DICT_4X4_100\n",
    ", draw_tag=False, white_balance=True):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Undistort the image\n",
    "    h, w = img.shape[:2]\n",
    "    new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(\n",
    "        cameraMatrix, distCoeffs, (w, h), 1, (w, h)\n",
    "    )\n",
    "    undistorted_image = cv2.undistort(\n",
    "        img, cameraMatrix, distCoeffs, None, new_camera_matrix\n",
    "    )\n",
    "\n",
    "    # Crop the image (if desired, based on ROI)\n",
    "    x, y, w, h = roi\n",
    "    img = undistorted_image[y : y + h, x : x + w]\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Load the dictionary that was used to generate the markers\n",
    "    aruco_dict = cv2.aruco.getPredefinedDictionary(aruco_dict_type)\n",
    "    # Initialize the detector parameters using default values\n",
    "    parameters = cv2.aruco.DetectorParameters()\n",
    "    detector = cv2.aruco.ArucoDetector(aruco_dict, parameters)\n",
    "\n",
    "    # Detect the markers in the image\n",
    "    corners, ids, rejected_img_points = cv2.aruco.detectMarkers(\n",
    "        gray, aruco_dict, parameters=parameters\n",
    "    )\n",
    "\n",
    "    if white_balance == True:\n",
    "        # Auto white balance\n",
    "        img = cv2.xphoto.createSimpleWB().balanceWhite(img)\n",
    "\n",
    "    if ids is not None:\n",
    "        # Assuming we have at least one marker detected, we will use the first detected marker\n",
    "        first_corners = corners[0]\n",
    "        corners = corners[0].reshape((4, 2))\n",
    "\n",
    "        # Define the destination points for the perspective transform\n",
    "        (top_left, top_right, bottom_right, bottom_left) = corners\n",
    "\n",
    "        width = max(\n",
    "            int(np.linalg.norm(bottom_right - bottom_left)),\n",
    "            int(np.linalg.norm(top_right - top_left)),\n",
    "        )\n",
    "        height = max(\n",
    "            int(np.linalg.norm(top_right - bottom_right)),\n",
    "            int(np.linalg.norm(top_left - bottom_left)),\n",
    "        )\n",
    "\n",
    "        dst_pts = np.array(\n",
    "            [\n",
    "                [img.shape[1] - (width - 1), img.shape[0] / 4],\n",
    "                [img.shape[1], img.shape[0] / 4],\n",
    "                [img.shape[1], (img.shape[0] / 4) + (height - 1)],\n",
    "                [img.shape[1] - (width - 1), (img.shape[0] / 4) + (height - 1)],\n",
    "            ],\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "        # Get the perspective transform matrix\n",
    "        M = cv2.getPerspectiveTransform(corners, dst_pts, cv2.WARP_INVERSE_MAP)\n",
    "\n",
    "        if draw_tag == True:\n",
    "            # Draw the markers on the frame\n",
    "            cv2.aruco.drawDetectedMarkers(img, [first_corners], ids[0])\n",
    "            \n",
    "        # Apply the perspective transformation to get the rectified image\n",
    "        rectified_img = cv2.warpPerspective(\n",
    "            img, M, (img.shape[1], img.shape[0]), flags=cv2.INTER_LINEAR\n",
    "        )\n",
    "\n",
    "        return rectified_img\n",
    "    else:\n",
    "        print(\"No ArUco markers detected.\")\n",
    "        return undistorted_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf7658-7d24-4c44-b5ec-87552cd4f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predefined dictionary\n",
    "aruco_dict = cv2.aruco.DICT_4X4_1000\n",
    "dictionary = cv2.aruco.getPredefinedDictionary(aruco_dict)\n",
    "parameters = cv2.aruco.DetectorParameters()\n",
    "detector = cv2.aruco.ArucoDetector(dictionary, parameters)\n",
    "\n",
    "# Measurement images containing charuco tag\n",
    "charuco_measurement_images = sorted(\n",
    "    glob.glob(\n",
    "        \"/media/fritz/01_PNG_DATA/MeasurementImages/28042025/labeled/*.JPG\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Output dir\n",
    "output_dir = \"/media/fritz/01_PNG_DATA/MeasurementImages/28042025/undistored_rectified/\"\n",
    "if os.path.isdir(output_dir) == False:\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Multical Calibration Parameters\n",
    "f = open(\n",
    "    \"/media/fritz/01_PNG_DATA/MeasurementImages/calibration_images/tg-6/calibration.json\"\n",
    ")\n",
    "data = json.load(f)\n",
    "cameraMatrix = np.array(data[\"cameras\"][\"cam0\"][\"K\"])\n",
    "distCoeffs = np.array(data[\"cameras\"][\"cam0\"][\"dist\"]).reshape(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177581d7-cd62-4f1c-927f-40efac10f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in charuco_measurement_images:\n",
    "    frame = cv2.imread(img)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect markers in the image\n",
    "    # corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(gray, aruco_dict, parameters=parameters)\n",
    "    corners, ids, rejectedImgPoints = detector.detectMarkers(gray)\n",
    "\n",
    "    # If markers are detected\n",
    "    if ids is not None:\n",
    "        rectified_image = rectify_image_using_aruco(img, cameraMatrix, distCoeffs, draw_tag = True, white_balance = True)\n",
    "    else:\n",
    "        rectified_image = rectify_image_using_aruco(img, cameraMatrix, distCoeffs, white_balance = True)\n",
    "\n",
    "    ## Save to file\n",
    "    filename = output_dir + Path(img).stem + \"_rectified\" + Path(img).suffix\n",
    "    cv2.imwrite(filename, rectified_image)\n",
    "\n",
    "    cv2.namedWindow(\"Frame\", cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(\"Frame\", rectified_image)\n",
    "    cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7482fa7-f64e-48cb-9d72-6a206c818136",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a0d42c-bdc2-47ff-9c3d-aa8b6f62f180",
   "metadata": {},
   "source": [
    "## Foreground Background Segmentation\n",
    "\n",
    "This is experimental, but in very good images the objects can directly be segmented from the background using this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b843893e-7e81-4690-94ab-6852c34dc9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from rembg import remove\n",
    "\n",
    "input_path = \"/media/fritz/01_PNG_DATA/MeasurementImages/28042025/undistored_rectified/RB51_Rank1_L_P1030213_00212_rectified.JPG\"\n",
    "output_path = \"/home/fritz/Pictures/fgbgTest.JPG\"\n",
    "\n",
    "img = cv2.imread(input_path)\n",
    "output = remove(img)\n",
    "\n",
    "# Save Image\n",
    "# cv2.imwrite(output_path, output)\n",
    "\n",
    "# Show image\n",
    "cv2.namedWindow(\"Foreground Background Image\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"Foreground Background Image\", output)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6382d-8d2c-4981-afab-80a7da354ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
