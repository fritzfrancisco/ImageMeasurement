{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87cca551-c401-4bd3-9555-b2cfed1e80e6",
   "metadata": {},
   "source": [
    "# Image Undistortion using Charuco Board "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40657ab9-20d5-45b4-808a-26e82eef3891",
   "metadata": {},
   "source": [
    "## Multical\n",
    "\n",
    "We will used [multical](https://github.com/oliver-batchelor/multical) to retrieve the calibration values for the camera. It is recommended to create a separate ```conda``` environment which we will call ```mcal```. This way the calibration software is contained and isn't affected by other packages.   \n",
    "\n",
    "To do so we first need to follow a few steps:\n",
    "\n",
    "1. **Create Calibration Pattern:** <br> To do so you can use the accompanied ```checkerboard_charuco_8x11.yaml``` file and running the following command in a terminal: <br> ```multical boards --boards checkerboard_charuco_8x11.yaml --write ~/Documents/ --paper_size_mm 216x279```  <br> This will create a calibration pattern which is recognized by [multical](https://github.com/oliver-batchelor/multical). It is advised to keep the ```checkerboard_charuco_8x11.yaml``` in a known location as it is used for the calibration process later on.\n",
    "2. **Take Pictures of Calibration Pattern:**<br> Now we need to take pictures with the camera that will be used for measuring. Ideally this is done under the same conditions as when measuring. Therefore, if measurement images are taken underwater, with the camera in a casing, the calibration images should be taken under the same circumstances and with ideantical setup. This ensures that the calibration is optimal!\n",
    "3. **Calibrate Camera:** <br> Once we have the calibration images, these should be stored in a folder with standardized name. Here we will use a folder structure of ```DDMMYYYY/cam1``` (for example: ```31072024/cam1```). Then we can run the calibration for a single camera using [multical](https://github.com/oliver-batchelor/multical) and the following command: <br>```multical calibrate --image_path /home/fritz/Pictures/olympus_undistortion_test/data/checkerboard_images/31072024/ --boards /home/fritz/Downloads/charuco_png_test_8x11.yaml --camera_pattern cam1``` <br> This command will create a few output files in the image directory, of which we will use ```calibration.json``` moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65410c70-fd9e-4650-bbf1-72d1fa095fd7",
   "metadata": {},
   "source": [
    "## Read out saved values\n",
    "\n",
    "newcameraMatrix, distCoeffs, rvecs, tvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ca51aa-6e23-4224-b543-70edf2e376a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cv2 import aruco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ace70e-e6dc-42ed-ba24-81df74b29f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Multical Output\n",
    "f = open(\n",
    "    \"/home/fritz/Pictures/olympus_undistortion_test/data/checkerboard_images/31072024/calibration.json\"\n",
    ")\n",
    "data = json.load(f)\n",
    "cameraMatrix = np.array(data[\"cameras\"][\"cam1\"][\"K\"])\n",
    "distCoeffs = np.array(data[\"cameras\"][\"cam1\"][\"dist\"]).reshape(1, 5)\n",
    "\n",
    "## Pattern for finding images to be undistorted\n",
    "filename_glob_pattern = (\n",
    "    \"/home/fritz/Pictures/olympus_undistortion_test/data/checkerboard_images/31072024/cam1/*.JPG\"\n",
    ")\n",
    "\n",
    "## Output directory for undistorted images\n",
    "output_dir = \"/home/fritz/Pictures/olympus_undistortion_test/data/checkerboard_images/data/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d750e8-6678-43f3-acc6-77893e1ab4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob.glob(filename_glob_pattern)\n",
    "\n",
    "cv2.namedWindow(\"Undistorted Image\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "if os.path.isdir(output_dir) == False:\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for img in images:\n",
    "    filename = output_dir + Path(img).stem + \"_undistorted\" + Path(img).suffix\n",
    "    print(filename)\n",
    "    img = cv2.imread(img)\n",
    "\n",
    "    # Undistort the image\n",
    "    h, w = img.shape[:2]\n",
    "    new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(\n",
    "        cameraMatrix, distCoeffs, (w, h), 1, (w, h)\n",
    "    )\n",
    "    undistorted_image = cv2.undistort(\n",
    "        img, cameraMatrix, distCoeffs, None, new_camera_matrix\n",
    "    )\n",
    "\n",
    "    # Crop the image (if desired, based on ROI)\n",
    "    x, y, w, h = roi\n",
    "    undistorted_image = undistorted_image[y : y + h, x : x + w]\n",
    "\n",
    "    # Save undistorted images to new directory\n",
    "    # Uncomment this line to save undistorted images to output directory\n",
    "    # cv2.imwrite(filename, undistorted_image)\n",
    "    \n",
    "    cv2.imshow(\"Undistorted Image\", undistorted_image)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23cad1b-f142-4935-a147-b1166e3ec9c6",
   "metadata": {},
   "source": [
    "## Warp Transform using Aruco Tag\n",
    "This process is also known as rectification and uses a reference shape to transform the image. It is important that this step is done on undistorted images, to reduce error and improve the process. The general process goes as follows:\n",
    "\n",
    "1. An Aruco Tag is attached to a flat surface on which the objects (i.e. Fish) are photographed and later measured. This tag should always be in full view, as well as the objects in the images. It is important to note the tag number and ideally the dictionary with which it was generated with. \n",
    "2. We can then tranform the image to a be as if it was viewed directly from above. This is done by automatically finding the tag in the image and warp transforming it to make the tag square again.\n",
    "3. Once transformed, the images can be saved for use in measuring the objects of interest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23509b3a-38b7-4d07-bbee-031d718e5d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "## Custom Function\n",
    "def rectify_image_using_aruco(\n",
    "    image_path, cameraMatrix, distCoeffs, aruco_dict_type=cv2.aruco.DICT_4X4_100\n",
    "):\n",
    "    '''Function to perform rectification of the image using OpenCV aruco tags'''\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Undistort the image\n",
    "    h, w = img.shape[:2]\n",
    "    new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(\n",
    "        cameraMatrix, distCoeffs, (w, h), 1, (w, h)\n",
    "    )\n",
    "    undistorted_image = cv2.undistort(\n",
    "        img, cameraMatrix, distCoeffs, None, new_camera_matrix\n",
    "    )\n",
    "\n",
    "    # Crop the image (if desired, based on ROI)\n",
    "    x, y, w, h = roi\n",
    "    img = undistorted_image[y : y + h, x : x + w]\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Load the dictionary that was used to generate the markers\n",
    "    aruco_dict = cv2.aruco.getPredefinedDictionary(aruco_dict_type)\n",
    "    # Initialize the detector parameters using default values\n",
    "    parameters = cv2.aruco.DetectorParameters()\n",
    "    detector = cv2.aruco.ArucoDetector(aruco_dict, parameters)\n",
    "\n",
    "    # Detect the markers in the image\n",
    "    corners, ids, rejected_img_points = cv2.aruco.detectMarkers(\n",
    "        gray, aruco_dict, parameters=parameters\n",
    "    )\n",
    "\n",
    "    if ids is not None:\n",
    "        # Assuming we have at least one marker detected, we will use the first detected marker\n",
    "        corners = corners[0].reshape((4, 2))\n",
    "\n",
    "        # Define the destination points for the perspective transform\n",
    "        (top_left, top_right, bottom_right, bottom_left) = corners\n",
    "\n",
    "        width = max(\n",
    "            int(np.linalg.norm(bottom_right - bottom_left)),\n",
    "            int(np.linalg.norm(top_right - top_left)),\n",
    "        )\n",
    "        height = max(\n",
    "            int(np.linalg.norm(top_right - bottom_right)),\n",
    "            int(np.linalg.norm(top_left - bottom_left)),\n",
    "        )\n",
    "\n",
    "        dst_pts = np.array(\n",
    "            [\n",
    "                [img.shape[1] - (width - 1), img.shape[0] / 4],\n",
    "                [img.shape[1], img.shape[0] / 4],\n",
    "                [img.shape[1], (img.shape[0] / 4) + (height - 1)],\n",
    "                [img.shape[1] - (width - 1), (img.shape[0] / 4) + (height - 1)],\n",
    "            ],\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "        # Get the perspective transform matrix\n",
    "        M = cv2.getPerspectiveTransform(corners, dst_pts, cv2.WARP_INVERSE_MAP)\n",
    "\n",
    "        # Apply the perspective transformation to get the rectified image\n",
    "        rectified_img = cv2.warpPerspective(\n",
    "            img, M, (img.shape[1], img.shape[0]), flags=cv2.INTER_LINEAR\n",
    "        )\n",
    "\n",
    "        return rectified_img\n",
    "    else:\n",
    "        print(\"No ArUco markers detected.\")\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90cf7658-7d24-4c44-b5ec-87552cd4f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predefined dictionary\n",
    "aruco_dict = cv2.aruco.DICT_4X4_1000\n",
    "dictionary = cv2.aruco.getPredefinedDictionary(aruco_dict)\n",
    "parameters = cv2.aruco.DetectorParameters()\n",
    "detector = cv2.aruco.ArucoDetector(dictionary, parameters)\n",
    "\n",
    "# Measurement images containing charuco tag\n",
    "charuco_measurement_images = sorted(\n",
    "    glob.glob(\n",
    "        \"/home/fritz/Pictures/olympus_undistortion_test/data/aruco_measurement_images/Tag_0/labeled/*.JPG\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Output dir\n",
    "output_dir = \"/home/fritz/Pictures/olympus_undistortion_test/data/aruco_measurement_images/Tag_0/undistored_rectified/\"\n",
    "if os.path.isdir(output_dir) == False:\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Multical Calibration Parameters\n",
    "f = open(\n",
    "    \"/home/fritz/Pictures/olympus_undistortion_test/data/checkerboard_images/31072024/calibration.json\"\n",
    ")\n",
    "data = json.load(f)\n",
    "cameraMatrix = np.array(data[\"cameras\"][\"cam1\"][\"K\"])\n",
    "distCoeffs = np.array(data[\"cameras\"][\"cam1\"][\"dist\"]).reshape(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177581d7-cd62-4f1c-927f-40efac10f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in charuco_measurement_images:\n",
    "    frame = cv2.imread(img)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect markers in the image\n",
    "    # corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(gray, aruco_dict, parameters=parameters)\n",
    "    corners, ids, rejectedImgPoints = detector.detectMarkers(gray)\n",
    "\n",
    "    # If markers are detected\n",
    "    if ids is not None:\n",
    "        # Draw the markers on the frame\n",
    "        cv2.aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "        rectified_image = rectify_image_using_aruco(img, cameraMatrix, distCoeffs)\n",
    "\n",
    "        ## Save to file\n",
    "        ## Uncomment this line to save rectified images to output directory\n",
    "        # filename = output_dir + Path(img).stem + \"_rectified\" + Path(img).suffix\n",
    "        # cv2.imwrite(filename, rectified_image)\n",
    "\n",
    "        cv2.namedWindow(\"Frame\", cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(\"Frame\", rectified_image)\n",
    "        cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7482fa7-f64e-48cb-9d72-6a206c818136",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a43ed-7f95-4055-9bd7-93f79ff83da9",
   "metadata": {},
   "source": [
    "## Foreground Background Segmentation\n",
    "\n",
    "This is experimental, but in very good images the objects can directly be segmented from the background using this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0542fde8-96d6-4b2d-92a9-ae6f930a7889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rembg import remove\n",
    "import cv2\n",
    "\n",
    "input_path = '/home/fritz/Pictures/olympus_undistortion_test/data/rectified_images/P7280145_rectified.JPG'\n",
    "output_path = '/home/fritz/Pictures/fgbgTest.JPG'\n",
    "\n",
    "input = cv2.imread(input_path)\n",
    "output = remove(input)\n",
    "cv2.imwrite(output_path, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b41bd2-e72b-4a09-8019-ad7726b1c0de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
